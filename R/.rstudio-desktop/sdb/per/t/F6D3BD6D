{
    "collab_server" : "",
    "contents" : "# labs\nlibrary(tidyverse)\nlibrary(readxl)\n\n\n\n## Chapter 7\nPLE_path <- file.path(\"Performance_Lawn_Equipment_Database.xlsx\")\nexcel_sheets(\"Performance_Lawn_Equipment_Database.xlsx\")\nCustomerSurvey2014 <- read_excel(PLE_path, sheet = \"2014 Customer Survey\", skip = 2)\n# no NAs\n\n# q1\n# CustomerSurvey2014 = read.csv(file = \"CustomerSurvey2014.csv\", stringsAsFactors = FALSE)\n# CustomerSurvey2014[is.na(CustomerSurvey2014)] <- \"NorthA\"\n# countByRegion <- table(CustomerSurvey2014$Region, CustomerSurvey2014$Quality)\n# countByRegion\nattach(CustomerSurvey2014)\n# CustomerSurvey2014 <- CustomerSurvey2014 %>% as_tibble\n# Quality <- CustomerSurvey2014 %>% select(Quality)\n# Quality <- CustomerSurvey2014$Quality\n# EasyOfUse <- CustomerSurvey2014 %>% select(Ease.of.Use)\n# EasyOfUse <- CustomerSurvey2014$Ease.of.Use\n# Service <- CustomerSurvey2014 %>% select(Service)\n\n# Data <- tibble(Y = c(Quality,EasyOfUse, Price, Service))\n# \n\nCustomerSurvey2014 %>% gather(key = c(\"Y\", Quality), value = c(Easy.of.Use, Price, Service))\n\n\n\nData <- data.frame( Y= c(Quality,`Ease of Use`,Price, Service),\n                    para = factor(rep(c(\"Quality\", \"Ease.of.Use\", \"Price\", \"Service\"),\n                                       times = c(length(Quality), length(Ease.of.Use),\n                                                 length(Price), length(Service)))))\n# apply anova\n# use function aov\nflm <- aov(Y ~ para, data = Data)\nanova(flm)\n\n\n\n# differences in ratings of specific product/service attributes in the \n# 2014 customer survey.csv data file\n\n\n# q2 ontime delivery\nOnTimeDelivery <- read_excel(PLE_path, sheet = \"On-Time Delivery\", skip = 2)\nOnTimeDelivery <- OnTimeDelivery %>% attach\nattach(OnTimeDelivery)\np0 <- sum(`Number On Time`[1:12]/sum(`Number of deliveries`[1:12]))\npbar <- sum(`Number On Time`[49:60]/sum(`Number of deliveries`[49:60]))\nn <- 12\nalpha <- 0.05\nqnorm(alpha)\nz.alpha <- qnorm(alpha)\npnorm()\n\n\n# find the proportion of “Top box”(Level 4 or 5) responses by region. \ntop_response <- apply(countByRegion, 1, function(x) sum(x[4], x[5]))\ntotal_response <- apply(countByRegion, 1, sum)\ntop_propotion <- top_response/total_response\ntop_propotion\n\n## Step 2\nResponseTime <- read.csv(\"ResponseTime.csv\")\nQ1.13mean <- mean(ResponseTime$Q1.2013)\nQ1.13std <- sd(ResponseTime$Q1.2013)\nQ1.13mean\nQ1.13std\n\n# Compute the z-test for first and second quarter. I guess \"first and second quarter\"\n# means first and second quarter in 2013\nlibrary(BSDA)\nattach(ResponseTime)\n\n# one sample z test\nz.test(Q1.2013, sigma.x = Q1.13std, mu = Q1.13mean)\nz.test(Q2.2013, sigma.x = sd(Q2.2013), mu = mean(Q2.2013))\n\n# 2 sample z test\nz.test(Q1.2013, Q2.2013, sigma.x = sd(Q1.2013), sigma.y = sd(Q2.2013))\n# What estimates, with reasonable assurance, can PLE give customers for response \n# times to customer service calls?\n\n# based on the result, we accept the null hypothesis, which means the response time for\n# first and seconde quarter of year 2013 are both normal distrubuted with the same mean.\n# So we can use 95% confidence interval of first quarter of year 2013, [3.504362, 4.326038]\n# as the estimates to give customers for response times to customer service calls.\n\n## Step 3\nPLE_path <- file.path(\"Performance_Lawn_Equipment_Database.xlsx\")\nexcel_sheets(\"Performance_Lawn_Equipment_Database.xlsx\")\nTransmissionCosts <- read_excel(PLE_path, sheet = \"Transmission Costs\", skip = 2)\ncurrent.sd <- sd(TransmissionCosts$Current)\ncurrent.mean <- mean(TransmissionCosts$Current)\nz.test(TransmissionCosts$Current, sigma.x = current.sd, mu = current.mean)\n\n\n# apply z.test for other 2 processes\nattach(TransmissionCosts)\n# names(TransmissionCosts)\nz.test(`Process A`, Current, sigma.x = mean(`Process A`), sigma.y = mean(Current))\nz.test(`Process B`, Current, sigma.x = mean(`Process B`), sigma.y = mean(Current))\n\n# based on the resulets, we accept the null hypothesis, which means \n# the Process A and Process B is not better than current process\n\n\n## Step 4\n\nMowerTest <- read_excel(PLE_path, sheet = \"Mower Test\", skip = 3)\nMowerTest <- MowerTest %>% mutate_all(as.factor)\nattach(MowerTest)\n# Convert the Pass/Fail levels to numeric values\n\nB <- MowerTest %>% group_by(Observation) %>% \n  mutate_all(function(x) ifelse(x == \"Pass\", 1, 0)) %>% \n  mutate_all(as.numeric)\n\n# Total number of failures:\nlength(which(B == 0))\n# In the instructions is 55, don't know why. Did you take the numbers in\n# \"Observations\" into count?\n\n# calculate mean of B\nmeanB <- B %>% ungroup %>% select(-Observation) %>% apply(1, mean) %>% mean\n\n# use t test on B\nB <- B %>% ungroup %>% select(-Observation) %>% apply(1, mean)\nt.test(B, mu = meanB)\n\n# based on the result, we accept the null hypothesis. So a 95% confidence interval for \n# an additional sample of mower test performance as in the worksheet Mower Test\n# is [,]\n\n\n# Step 5\nBladeWeight <- read_excel(PLE_path, \"Blade Weight\", skip = 2)\nhist(BladeWeight$Weight)\nmean(BladeWeight$Weight)\n# Compute the standard error of the mean for our blade weight data using given formula:\n# well, didn't see any formula.\nattach(BladeWeight)\nsem <- sd(Weight)/sqrt(length(Weight))\nsem\n\n# test normality\nA <- lm(BladeWeight$Weight ~ ., BladeWeight)\nplot(A)\n\n# from the plot we can see there is a significant outlier 171. get rid of it\nqqnorm(Weight[-171])\nqqline(Weight[-171])\n\n# seems not normally distributed. \n# to be more sure, run a normality test:\nshapiro.test(Weight[-171])\n# based on result we should reject the null hypothesis.\n\n\n# Step 6\n\n\nx# to find how the response differs by quarter we can look at our descriptive \n# statistics and create a plot of the mean. \n\n# means of the quarters\nmResponseTime <- c(\n    mean(ResponseTime$Q1.2013),\n    mean(ResponseTime$Q2.2013),\n    mean(ResponseTime$Q3.2013),\n    mean(ResponseTime$Q4.2013),\n    mean(ResponseTime$Q1.2014),\n    mean(ResponseTime$Q2.2014),\n    mean(ResponseTime$Q3.2014),\n    mean(ResponseTime$Q4.2014)\n  )\n\nmResponseTime\n\n# plot mResponseTime. define xaxis labels as the columns names of ResponseTime\nplot(mResponseTime, type = \"l\", lwd=2, col=\"green\", ylab = \"Mean\", xlab = \"Quarter\", main=\"Mean Response\nTime\", ylim = c(0,5), xaxt=\"n\")\naxis(side = 1, at = c(1:8), labels = names(ResponseTime), pch=0.5)\n\n\n\nDealerSatisfaction = read.csv(file = \"DealerSatisfaction.csv\")\n# str(DealerSatisfaction)\ndealerSat_NA <- DealerSatisfaction[1:5, ]\ndealerSat_NA\ntdealerSat_NA <- t(dealerSat_NA[,3:8])\ntdealerSat_NA\ncolnames(tdealerSat_NA) <- c(\"2010\", \"2011\", \"2012\", \"2013\", \"2014\")\ntdealerSat_NA\n\n# brute force approach to computing the mean for North America for the year 2010\nm_NA2010 <- ((tdealerSat_NA[2,1]*1) + (tdealerSat_NA[3,1]*2) + (tdealerSat_NA[4,1]*3) +\n               (tdealerSat_NA[5,1]*4) + (tdealerSat_NA[6,1]*5))/sum(tdealerSat_NA[,1])\nm_NA2010\n\n# avoiding NAs by read columns as characters\nDealerSatisfaction = read.csv(file = \"DealerSatisfaction.csv\", stringsAsFactors = F)\nDealerSatisfaction[is.na(DealerSatisfaction)] <- \"NorthA\"\n\n#  mean values for all the regions for all the years in a vector m\nm=0\nfor (j in 1:25){\n  # print(j)\n  m[j] <- ((DealerSatisfaction[j,4]*1) + (DealerSatisfaction[j,5]*2) +\n             (DealerSatisfaction[j,6]*3) + (DealerSatisfaction[j,7]*4)\n           +(DealerSatisfaction[j,8]*5))/sum(DealerSatisfaction[j,3:8])\n  # print(m[j])\n}\nprint(m)\n\n# get m in a matrix\nn <- matrix(m, 5, byrow=FALSE)\nn\n\n# calculation for standard deviation using the mean.\nm=0\nn=0\nfor (j in 1:25){\n  # print(j)\n  m[j] <- ((DealerSatisfaction[j,4]*1) + (DealerSatisfaction[j,5]*2) +\n             (DealerSatisfaction[j,6]*3) + (DealerSatisfaction[j,7]*4)\n           +(DealerSatisfaction[j,8]*5))/sum(DealerSatisfaction[j,3:8])\n  n[j] <- sqrt(((DealerSatisfaction[j,3] * (0 - m[j])^2) +\n                  (DealerSatisfaction[j,4] * (1 - m[j])^2) +\n                  (DealerSatisfaction[j,5] * (2 - m[j])^2) +\n                  (DealerSatisfaction[j,6] * (3 - m[j])^2) +\n                  (DealerSatisfaction[j,7] * (4 - m[j])^2) +\n                  (DealerSatisfaction[j,8] * (5 - m[j])^2))/\n                 (sum(DealerSatisfaction[j,3:8])-1))\n  # print(m[j])\n  # print(n[j])\n}\nprint(m)\nprint(n)\n\n# write in a matrix\np <- matrix(n, 5, byrow = FALSE)\np\n\n# Now we have a process you can calculate the values for End-User Satisfaction. \nEndUserSatisfication = read.csv(file = \"EndUserSatisfication.csv\", stringsAsFactors = F)\nEndUserSatisfication[is.na(EndUserSatisfication)] <- \"NorthA\"\n\n# use the same \"brute\" force approach to get mean and standard deviation\nm=0\nn=0\nfor (j in 1:25){\n  # print(j)\n  m[j] <- ((EndUserSatisfication[j,4]*1) + (EndUserSatisfication[j,5]*2) +\n             (EndUserSatisfication[j,6]*3) + (EndUserSatisfication[j,7]*4)\n           +(EndUserSatisfication[j,8]*5))/sum(EndUserSatisfication[j,3:8])\n  n[j] <- sqrt(((EndUserSatisfication[j,3] * (0 - m[j])^2) +\n                  (EndUserSatisfication[j,4] * (1 - m[j])^2) +\n                  (EndUserSatisfication[j,5] * (2 - m[j])^2) +\n                  (EndUserSatisfication[j,6] * (3 - m[j])^2) +\n                  (EndUserSatisfication[j,7] * (4 - m[j])^2) +\n                  (EndUserSatisfication[j,8] * (5 - m[j])^2))/\n                 (sum(EndUserSatisfication[j,3:8])-1))\n  # print(m[j])\n  # print(n[j])\n}\nprint(m)\nprint(n)\n\n# write in matrix\npm <- matrix(m, 5, byrow = FALSE)\npn <- matrix(n, 5, byrow = FALSE)\n\npm\npn\n\n# Step 2\n# use psych package\nCustomerSurvey2014 <- read.csv(\"CustomerSurvey2014.csv\")\n# subset the customer survey data by region in order to complete this part of the exercise.\ncustSurveyNA <- CustomerSurvey2014[1:100,-1]\ncustSurveySA <-CustomerSurvey2014[102:151,-1]\n\n# get mode of custSurveyNA and custSurveySA\nlibrary(plyr)\ngetmode <- function(v) {\n  uniqv <- unique(v)\n  uniqv[which.max(tabulate(match(v, uniqv)))]\n}\ncustSurveyNAmode <- apply(custSurveyNA, 2, getmode)\ncustSurveySAmode <- apply(custSurveySA, 2, getmode)\n\n# describe function in psych package\nlibrary(psych)\ndescribe(custSurveySA)\n\nlibrary(pastecs)\nstat.desc(custSurveySA)\n\n# Step 3\n\n# Step 4\nDefectsAfterDelivery <- read.csv(\"DefectsAfterDelivery.csv\")\nstr(DefectsAfterDelivery)\n\n# find the means of the defects after delivery over time and plot.\nm <- apply(DefectsAfterDelivery[,-1], 2, mean)\nplot(m, type = \"l\", lwd=2, col=\"green\", ylab = \"Mean\", xlab = \"Year\", \n     main=\"Mean Defects After Delivery\", xaxt=\"n\")\naxis(side = 1, at = c(1:length(m)), labels = names(m), pch=0.5)\n\n# Step 5\nMowerUnitSales <- read.csv(file = \"Mower Unit Sales.csv\")\nIndustryMowerTotalSales <- read.csv(\"IndustryMowerTotalSales.csv\")\n\nstr(MowerUnitSales)\nstr(IndustryMowerTotalSales)\n\n#  combine these data files keeping the month/year variables from the Mower Unit Sales data file. \ntotalMowerSales <- MowerUnitSales[,]\nstr(totalMowerSales)\ntotalMowerSales[,8:12]<- IndustryMowerTotalSales[,-1]\nstr(totalMowerSales)\ncolnames(totalMowerSales) <- c(\"Date\", \"NorthA\", \"SA\", \"Eur\", \"Pac\", \"China\", \"World\", \"IndustryNorthA\", \"IndustrySA\n\", \"IndustryEur\", \"IndustryPac\", \"IndustryWorld\")\nhead(totalMowerSales)\n\n# get the coefficient of variance for mower sales\nstat.desc(totalMowerSales)\n\n# To find the correlation table and simultaneously find the \n# significance of the correlations we’ll use the Hmisc package. \n# use the rcorr() function to get the correlation table and significance \nlibrary(Hmisc)\nrcorr(as.matrix(totalMowerSales[2:12]))\n# P values are very low, essentially 0, the correlations are statistically significant\n\n# follow this same procedure for tractor sales. \nTractorUnitSales = read.csv(file = \"Tractor Unit Sales.csv\")\nIndustryTractorTotalSales <- read.csv(\"IndustryTractorTotalSales.csv\")\n\n#  combine these data files keeping the month/year variables from the Tractor Unit Sales data file. \ntotalTractorSales <- TractorUnitSales[,]\n# str(totalTractorSales)\ntotalTractorSales[,8:12]<- IndustryTractorTotalSales[,-1]\nstr(totalTractorSales)\ncolnames(totalTractorSales) <- c(\"Date\", \"NorthA\", \"SA\", \"Eur\", \"Pac\", \"China\", \"World\", \"IndustryNorthA\", \"IndustrySA\n                               \", \"IndustryEur\", \"IndustryPac\", \"IndustryWorld\")\n# head(totalTractorSales)\n\n# get the coefficient of variance for mower sales\nstat.desc(totalTractorSales)\n# get correlation table\nrcorr(as.matrix(totalTractorSales[2:12]))\n\n# We can see that the P values are still very low, essentially 0, \n# so the correlations are statistically significant\n\n# Chapter 5\n# part 1\n\n# step 1: Bernoulli Distribution. \n\n# step 2: \nMowerTest <- read.csv(\"MowerTest.csv\")\ncountFail <- length(which(MowerTest == \"Fail\"))\ncountFail \n# the probability of “Fail” is 54/3000 = 0.018. \n\n# step 3\n#  find the probability of having from 0 to 20 failures in the next 100 mowers\n# tested. use binomial distribution. function: dbinom()\ny <- dbinom(0:20, 100, .018)\ny\nplot(y)\n\n# step 4\nBladeWeight <- read.csv(\"BladeWeight.csv\")\nstr(BladeWeight)\n(sum(BladeWeight$Weight)/350)\nsd(BladeWeight$Weight)\n# So, we could expect the blade weight to be 4.99 +/- 2*0.11. Note that we’ve rounded the standard\n# deviation and assumed a 2-tail solution via the empirical rules\n\n# step 5\n# determine the probability that the blade weight can exceed 5.20. To do this we use\n# the pnorm() function for the Normal Distribution\ny = pnorm(5.20, mean=4.99, sd=0.11)\ny\n1-y\n\n# step 6\n# determine the probability that the blade weight will be less than 4.80. Again, we’ll\n# use the pnorm() function\npnorm(4.80, mean=4.99, sd=0.11)\n\n# step 7\n# find the number of blades that exceeded 5.20 or were less than 4.80 from the data.\n# use the length() function\ncountblade <- length(which(BladeWeight$Weight > 5.20))\ncountblade\ncountblade2 <- length(which(BladeWeight$Weight <= 4.80))\ncountblade2\n\n# got a different answer if set the test to be greater than or equal to using >= logical operator. \n# Likewise I could have gotten a different answer if I had use < rather than <= as\n# the logical operator in the second computation.\n\n# step 8\n# examine, over time, the process that makes the blades by considering changes in\n# blade weights over time. We can just plot the blades manufactured to see if there is any variation over\n# time. We can use the plot() function to generate a scatterplot to look at this as follows:\n\nplot(BladeWeight$Sample, BladeWeight$Weight)\n# From the scatterplot it doesn’t look like there is too much variation about the average blade weight of\n# 4.99. \n\n# step 9\n# Looking at the scatter plot, only close to the 200th blade was there any trouble. It isn’t too much trouble\n# to find that this is blade #171. \n\n# step 10\n# checkif the normal distribution is a good assumption for the blade weight data\n#  plot a histogram of the data. hist() function\nhist(BladeWeight$Weight, prob = T, ylim = c(0,4))\nlines(density(BladeWeight$Weight))\n# add a density estimate with defaults\nlines(density(BladeWeight$Weight, adjust=2), lty=\"dotted\") \n# add another \"smoother\" density\n\nMower_NA <- MowerUnitSales[1:5, ]\nMower_NA\n\ntdealerSat_NA <- t(dealerSat_NA[,3:8])\ntdealerSat_NA\ncolnames(tdealerSat_NA) <- c(\"2010\", \"2011\", \"2012\", \"2013\", \"2014\")\ntdealerSat_NA\n\nlibrary(reshape2)\ndata.m2 <- melt(tdealerSat_NA, id.vars=var1)\ndata.m2\ncolnames(data.m2) <- c(\"Level\", \"Year\", \"Counts\")\ndata.m2\nggplot(data.m2, aes(x=Year, y=Counts)) + \n  geom_bar(aes(fill=Level), position=\"dodge\", stat=\"identity\")\n\nggplot(data.m2, aes(x=Year, y=Counts, fill=Level)) + geom_bar(stat=\"identity\")\n```\n\n### Step 2\n```{r P1S2, echo=T}\nComplaints = read.csv(file = \"Complaints.csv\")\n# \"brute force\" method (using plot())\nplot(Complaints$World, ylim=range(c(0,400)), type=\"l\", xlab=\"Month\", ylab=\"Number of Complaints\")\npar(new=TRUE)\nplot(Complaints$NA., ylim=range(c(0,400)), type=\"l\", col=\"red\", axes = FALSE, xlab = \"\", ylab = \"\")\npar(new=TRUE)\nplot(Complaints$SA, ylim=range(c(0,400)), type=\"l\", col=\"green\", axes = FALSE, xlab = \"\", ylab = \"\")\npar(new=TRUE)\nplot(Complaints$Eur, ylim=range(c(0,400)), type=\"l\", col=\"blue\", axes = FALSE, xlab = \"\", ylab = \"\")\npar(new=TRUE)\nplot(Complaints$Pac, ylim=range(c(0,400)), type=\"l\", col=\"magenta\", axes = FALSE, xlab = \"\", ylab = \"\")\npar(new=TRUE)\nplot(Complaints$China, ylim=range(c(0,400)), type=\"l\", col=\"deeppink4\", axes = FALSE, xlab = \"\", ylab = \"\")\n```\n\n# You can use this or any other plotting methods, e.g. ggplot(), to create the line plots to\n# finish Part 1 of Chapter 3. \n\n### Part 2\n### Step 1\n```{r P2S1, echo=T}\nShippingCost = read.csv(file = \"ShippingCost.csv\")\nsummary(ShippingCost$Mowers)\nsummary(ShippingCost$Tractors)\n```\n\n### Part 3\n### Step 1\n```{r P3S1, echo=T}\nCustomerSurvey2014 = read.csv(file = \"CustomerSurvey2014.csv\")\nstr(CustomerSurvey2014)\nsummary(CustomerSurvey2014)\nrm(CustomerSurvey2014)\n\nCustomerSurvey2014 = read.csv(file = \"CustomerSurvey2014.csv\", stringsAsFactors = FALSE)\nCustomerSurvey2014[is.na(CustomerSurvey2014)] <- \"NorthA\"\nstr(CustomerSurvey2014)\nbarplot(table(CustomerSurvey2014$Region))\nhist(CustomerSurvey2014$Quality, main=\"Quality - Number of Responses\", xlab=\"Level of Quality\")\n```\n\n\n",
    "created" : 1500770888368.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3292633319",
    "id" : "F6D3BD6D",
    "lastKnownWriteTime" : 1503357331,
    "last_content_update" : 1503357331494,
    "path" : "~/Documents/Harrisburg/500_52/lab.R",
    "project_path" : null,
    "properties" : {
        "marks" : "<:30,0\n>:30,17",
        "tempName" : "Untitled4"
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}